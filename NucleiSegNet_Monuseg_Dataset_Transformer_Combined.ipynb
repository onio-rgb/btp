{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xoLrOHxN6CHJ"
      },
      "outputs": [],
      "source": [
        "# !pip install spams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRDU90IGOjWs",
        "outputId": "7f4738bb-b911-4679-c0aa-bb622c1c7eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchstain in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchstain) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchstain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an9NIVgxZtZy",
        "outputId": "87fade6a-6b40-4fe6-da3a-468be3d1526e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.enable_eager_execution(\n",
        "    config=None, device_policy=None, execution_mode=None\n",
        ")\n",
        "tf.executing_eagerly()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9HCf40oKvfQi"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall numpy\n",
        "# !pip install numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CEQVlUtr6wf",
        "outputId": "1f39d639-6566-488f-eab5-a5b78df8d96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ipMvPnvHY7K",
        "outputId": "ea9c6c9d-4b89-48ba-8039-046bb4091f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NucleiSegNet\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/NucleiSegNet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOzJPf3BHdAi",
        "outputId": "1167e096-b5c9-4f6d-e343-cae186a8105f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stain normalizing and cropping patches of train images and masks ... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [01:13<00:00,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stain normalizing and cropping patches of validation images and masks ... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 7/7 [00:14<00:00,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stain normalizing and cropping patches of test images ... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 7/7 [00:11<00:00,  1.70s/it]\n",
            "100%|██████████| 7/7 [00:00<00:00,  8.30it/s]\n",
            "100%|██████████| 7/7 [00:00<00:00,  8.04it/s]\n",
            "100%|██████████| 37/37 [00:03<00:00, 10.25it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# %matplotlib inline\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchstain\n",
        "# import spams\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "import math\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "M_CHANNEL=1\n",
        "Res_HEIGHT = 1000  # actual image height\n",
        "Res_WIDTH = 1000   # actual image width\n",
        "#no of patches = (input image size/ crop size)^2  per image .\n",
        "pat = 16\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
        "seed = 42\n",
        "np.random.seed = seed\n",
        "# path where you want to store the stain normalized images\n",
        "#----- # Test # ------#\n",
        "Path(\"/content/TestData\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"/content/TestData/Bin\").mkdir(parents=True, exist_ok=True) # for masks\n",
        "Path(\"/content/TestData/tis\").mkdir(parents=True, exist_ok=True) # for tissues\n",
        "\n",
        "bin_p_ts = '/content/TestData/Bin'\n",
        "tis_p_ts = '/content/TestData/tis'\n",
        "#----- # Train # ------#\n",
        "Path(\"/content/TrainData\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"/content/TrainData/Bin\").mkdir(parents=True, exist_ok=True) # for masks\n",
        "Path(\"/content/TrainData/tis\").mkdir(parents=True, exist_ok=True) # for tissues\n",
        "\n",
        "bin_p_tr = '/content/TrainData/Bin/'\n",
        "tis_p_tr = '/content/TrainData/tis/'\n",
        "#----- # Valid # ------#\n",
        "Path(\"/content/ValidData\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"/content/ValidData/Bin\").mkdir(parents=True, exist_ok=True) # for masks\n",
        "Path(\"/content/ValidData/tis\").mkdir(parents=True, exist_ok=True) # for tissues\n",
        "\n",
        "bin_p_vl = '/content/ValidData/Bin/'\n",
        "tis_p_vl = '/content/ValidData/tis/'\n",
        "\n",
        "# Give path to your dataset\n",
        "Train_image_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Train/data/'\n",
        "Train_mask_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Train/label/'\n",
        "\n",
        "val_image_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Validation/data/'\n",
        "val_mask_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Validation/label/'\n",
        "\n",
        "Test_image_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Test/data/'\n",
        "test_mask_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Test/label/'\n",
        "\n",
        "# Give a reference image path for stain normalization\n",
        "reference_image = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Test/data/TCGA-EJ-A46H-01A-03-TSC.tif'\n",
        "\n",
        "\n",
        "# getting the train and test ids\n",
        "train_ids1 = next(os.walk(Train_image_path))[2]\n",
        "train_mask_ids1 = next(os.walk(Train_mask_path))[2]\n",
        "val_ids1 = next(os.walk(val_image_path))[2]\n",
        "val_mask_ids1 = next(os.walk(val_mask_path))[2]\n",
        "test_ids1 = next(os.walk(Test_image_path))[2]\n",
        "test_mask_ids1 = next(os.walk(test_mask_path))[2]\n",
        "\n",
        "# sorting the train and test ids\n",
        "train_ids = sorted(train_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "train_mask_ids = sorted(train_mask_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "test_ids = sorted(test_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "test_mask_ids = sorted(test_mask_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "val_ids = sorted(val_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "val_mask_ids = sorted(val_mask_ids1,key=lambda x: (os.path.splitext(x)[0]))\n",
        "\n",
        "\n",
        "train_ids[:] = [tup for tup in train_ids if os.path.isfile(Train_mask_path + (os.path.splitext(tup)[0])+'.png')]\n",
        "test_ids[:] = [tup for tup in test_ids if os.path.isfile(test_mask_path + (os.path.splitext(tup)[0])+'.png')]\n",
        "val_ids[:] = [tup for tup in val_ids if os.path.isfile(val_mask_path + (os.path.splitext(tup)[0])+'.png')]\n",
        "\n",
        "def stain_norm_patch():\n",
        "\n",
        "    def read_image(path):\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv default color space is BGR, change it to RGB\n",
        "        p = np.percentile(img, 90)\n",
        "        img = np.clip(img * 255.0 / p, 0, 255).astype(np.uint8)\n",
        "        return img\n",
        "\n",
        "    def vaha(SOURCE_PATH,TARGET_PATH):\n",
        "        source_image = read_image(SOURCE_PATH)\n",
        "        target_image = read_image(TARGET_PATH)\n",
        "        vhd = vahadane(LAMBDA1=0.01, LAMBDA2=0.01, fast_mode=1, getH_mode=0, ITER=20)\n",
        "        # vhd.show_config()\n",
        "\n",
        "        Ws, Hs = vhd.stain_separate(source_image)\n",
        "        vhd.fast_mode=0;vhd.getH_mode=0;\n",
        "        Wt, Ht = vhd.stain_separate(target_image)\n",
        "        img = vhd.SPCN(source_image, Ws, Hs, Wt, Ht)\n",
        "        return img\n",
        "\n",
        "    # def rein(src):\n",
        "    #     # stain_normalizer 'Vahadane'\n",
        "    #     target_img = reference_image\n",
        "    #     im_nmzd = vaha(src,target_img)\n",
        "    #     return im_nmzd\n",
        "\n",
        "    def rein(src):\n",
        "      target_img = reference_image\n",
        "      target = cv2.cvtColor(cv2.imread(target_img), cv2.COLOR_BGR2RGB)\n",
        "      to_transform = cv2.cvtColor(cv2.imread(src), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      T = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Lambda(lambda x: x*255)\n",
        "      ])\n",
        "\n",
        "      torch_normalizer = torchstain.normalizers.MacenkoNormalizer(backend='torch')\n",
        "      torch_normalizer.fit(T(target))\n",
        "\n",
        "      t_to_transform = T(to_transform)\n",
        "      norm, H, E = torch_normalizer.normalize(I=t_to_transform, stains=True)\n",
        "      return norm\n",
        "\n",
        "    # Get and resize train images and masks\n",
        "    def train():\n",
        "        X_train = np.zeros((len(train_ids)*pat, IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)\n",
        "        Y_train = np.zeros((len(train_ids)*pat, IMG_HEIGHT, IMG_WIDTH,1), dtype=bool)\n",
        "        print('stain normalizing and cropping patches of train images and masks ... ')\n",
        "        sys.stdout.flush()\n",
        "        for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
        "            path = Train_mask_path + (os.path.splitext(id_)[0])+'.png'\n",
        "            img = rein(Train_image_path + id_)\n",
        "            mask_ = cv2.imread(path,0)\n",
        "            _, mask_ = cv2.threshold(mask_, 128, 255, cv2.THRESH_BINARY)\n",
        "            mask_ = np.expand_dims(mask_, -1)\n",
        "            temp_list = []\n",
        "            temp_list_mask = []\n",
        "            for i in range (int(math.pow(pat,0.5))):\n",
        "                for j in range(int(math.pow(pat,0.5))):\n",
        "                    if i<(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img1 = img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask1 = mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_img1)\n",
        "                            temp_list_mask.append(crop_mask1)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img2 = img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask2 = mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_img2)\n",
        "                            temp_list_mask.append(crop_mask2)\n",
        "                    elif i==(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img3 = img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask3 = mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_img3)\n",
        "                            temp_list_mask.append(crop_mask3)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img4 = img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask4 = mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_img4)\n",
        "                            temp_list_mask.append(crop_mask4)\n",
        "\n",
        "            for t in range(0,pat):\n",
        "                X_train[n*pat+t] = temp_list[t]\n",
        "                Y_train[n*pat+t] = temp_list_mask[t]\n",
        "                # mask = np.maximum(mask, mask_)\n",
        "        return X_train, Y_train\n",
        "\n",
        "\n",
        "    def val():\n",
        "        X_val = np.zeros((len(val_ids)*pat, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
        "        Y_val = np.zeros((len(val_ids)*pat, IMG_HEIGHT, IMG_WIDTH,1), dtype=bool)\n",
        "        print('stain normalizing and cropping patches of validation images and masks ... ')\n",
        "        sys.stdout.flush()\n",
        "        for m, id_ in tqdm(enumerate(val_ids), total=len(val_ids)):\n",
        "\n",
        "            path = val_mask_path + (os.path.splitext(id_)[0])+'.png'\n",
        "            val_img = rein(val_image_path + id_)\n",
        "            val_mask_ = cv2.imread(path,0)\n",
        "            _, val_mask_ = cv2.threshold(val_mask_, 128, 255, cv2.THRESH_BINARY)\n",
        "            val_mask_ = np.expand_dims(val_mask_, -1)\n",
        "\n",
        "            temp_list = []\n",
        "            temp_list_mask = []\n",
        "            for i in range (int(math.pow(pat,0.5))):\n",
        "                for j in range(int(math.pow(pat,0.5))):\n",
        "                    if i<(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_val_img1 = val_img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask1 = val_mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_val_img1)\n",
        "                            temp_list_mask.append(crop_mask1)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_val_img2 = val_img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask2 = val_mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_val_img2)\n",
        "                            temp_list_mask.append(crop_mask2)\n",
        "                    elif i==(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_val_img3 = val_img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask3 = val_mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_val_img3)\n",
        "                            temp_list_mask.append(crop_mask3)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_val_img4 = val_img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask4 = val_mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_val_img4)\n",
        "                            temp_list_mask.append(crop_mask4)\n",
        "\n",
        "            for t in range(0,pat):\n",
        "                X_val[m*pat+t] = temp_list[t]\n",
        "                Y_val[m*pat+t] = temp_list_mask[t]\n",
        "                # mask = np.maximum(mask, mask_)\n",
        "        return X_val, Y_val\n",
        "\n",
        "\n",
        "    def test():\n",
        "        X_test = np.zeros((len(test_ids)*pat, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
        "        Y_test = np.zeros((len(test_ids)*pat, IMG_HEIGHT, IMG_WIDTH,1), dtype=bool)\n",
        "        print('stain normalizing and cropping patches of test images ... ')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        for s, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
        "\n",
        "            path = test_mask_path + (os.path.splitext(id_)[0])+'.png'\n",
        "            img = rein(Test_image_path + id_)\n",
        "            test_mask_ = cv2.imread(path,0)\n",
        "            _, test_mask_ = cv2.threshold(test_mask_, 128, 255, cv2.THRESH_BINARY)\n",
        "            test_mask_ = np.expand_dims(test_mask_, -1)\n",
        "\n",
        "            temp_list = []\n",
        "            temp_list_mask = []\n",
        "            for i in range (int(math.pow(pat,0.5))):\n",
        "                for j in range(int(math.pow(pat,0.5))):\n",
        "                    if i<(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img1 = img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask1 = test_mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_img1)\n",
        "                            temp_list_mask.append(crop_mask1)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img2 = img[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask2 = test_mask_[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_img2)\n",
        "                            temp_list_mask.append(crop_mask2)\n",
        "                    elif i==(int(math.pow(pat,0.5))-1):\n",
        "                        if j<(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img3 = img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            crop_mask3 = test_mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH]\n",
        "                            temp_list.append(crop_img3)\n",
        "                            temp_list_mask.append(crop_mask3)\n",
        "                        elif j==(int(math.pow(pat,0.5))-1):\n",
        "                            crop_img4 = img[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            crop_mask4 = test_mask_[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24]\n",
        "                            temp_list.append(crop_img4)\n",
        "                            temp_list_mask.append(crop_mask4)\n",
        "\n",
        "            for t in range(0,pat):\n",
        "                X_test[s*pat+t] = temp_list[t]\n",
        "                Y_test[s*pat+t] = temp_list_mask[t]\n",
        "                # mask = np.maximum(mask, mask_)\n",
        "        return X_test, Y_test\n",
        "\n",
        "    train1 = train()\n",
        "    X_train = train1[0]\n",
        "    Y_train = train1[1]\n",
        "\n",
        "    val1 = val()\n",
        "    X_val = val1[0]\n",
        "    Y_val = val1[1]\n",
        "\n",
        "    test1 = test()\n",
        "    X_test = test1[0]\n",
        "    Y_test = test1[1]\n",
        "\n",
        "    # this will save the stain normalized patches into the created paths above\n",
        "    #------------------------#TEST#---------------------------------#\n",
        "\n",
        "    for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
        "        id_1 = os.path.splitext(id_)[0]\n",
        "        for j in range(pat):\n",
        "            j1 = \"{0:0=2d}\".format(j)\n",
        "            img_t = X_test[n*pat+j]\n",
        "            imgs_b = Y_test[n*pat+j]*255\n",
        "            # img_t = X_test[n]\n",
        "            # imgs_b = np.reshape(Y_test[n]*255,(IMG_WIDTH,IMG_HEIGHT))\n",
        "            filename1 = '{}/{}_{}.png'.format(tis_p_ts,id_1,j1)\n",
        "            cv2.imwrite(filename1, cv2.cvtColor(img_t, cv2.COLOR_BGR2RGB))\n",
        "            filename2 = '{}/{}_{}.png'.format(bin_p_ts,id_1,j1)\n",
        "            cv2.imwrite(filename2, imgs_b)\n",
        "    #------------------------#VAL#-------------------------------#\n",
        "\n",
        "    for n, id_ in tqdm(enumerate(val_ids), total=len(val_ids)):\n",
        "        id_1 = os.path.splitext(id_)[0]\n",
        "\n",
        "        for j in range(pat):\n",
        "            j1 = \"{0:0=2d}\".format(j)\n",
        "            img_t = X_val[n*pat+j]\n",
        "            imgs_b = Y_val[n*pat+j]*255\n",
        "            filename1 = '{}/{}_{}.png'.format(tis_p_vl,id_1,j1)\n",
        "            cv2.imwrite(filename1,cv2.cvtColor(img_t, cv2.COLOR_BGR2RGB))    #cv2.cvtColor(img_t, cv2.COLOR_BGR2RGB)\n",
        "            filename2 = '{}/{}_{}.png'.format(bin_p_vl,id_1,j1)\n",
        "            cv2.imwrite(filename2, imgs_b)\n",
        "    #------------------------#TRAIN#-------------------------------#\n",
        "\n",
        "    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
        "        id_1 = os.path.splitext(id_)[0]\n",
        "\n",
        "        for j in range(pat):\n",
        "            j1 = \"{0:0=2d}\".format(j)\n",
        "            img_t = X_train[n*pat+j]\n",
        "            imgs_b = Y_train[n*pat+j]*255\n",
        "            filename1 = '{}/{}_{}.png'.format(tis_p_tr,id_1,j1)\n",
        "            cv2.imwrite(filename1, cv2.cvtColor(img_t, cv2.COLOR_BGR2RGB))  #cv2.cvtColor(img_t, cv2.COLOR_BGR2RGB)\n",
        "            filename2 = '{}/{}_{}.png'.format(bin_p_tr,id_1,j1)\n",
        "            cv2.imwrite(filename2, imgs_b)\n",
        "\n",
        "def patch_join(out_im):\n",
        "    num_im = len(out_im)//pat\n",
        "    num_pat = int(pat**0.5)\n",
        "    out_concat = np.zeros((Res_HEIGHT, Res_WIDTH, 1), dtype=np.uint8)\n",
        "    # Y_concat = np.zeros((Res_HEIGHT, Res_WIDTH, 1), dtype=np.bool)\n",
        "\n",
        "    out_full = np.zeros((num_im,Res_HEIGHT, Res_WIDTH, 1), dtype=np.uint8)\n",
        "    # Y_full = np.zeros((num_im,Res_HEIGHT, Res_WIDTH, 1), dtype=np.bool)\n",
        "\n",
        "\n",
        "    for k in range(num_im):\n",
        "        sec1 = []\n",
        "        y_sec1 = []\n",
        "        for l in range(pat):\n",
        "\n",
        "            sec = out_im[k*pat+l]\n",
        "            sec1.append(sec)\n",
        "\n",
        "        for i in range(int(num_pat)):\n",
        "            for j in range(int(num_pat)):\n",
        "\n",
        "                if i<num_pat-1:\n",
        "                    if j<num_pat-1:\n",
        "                        out_concat[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH] = sec1[i*num_pat+j]\n",
        "\n",
        "                    elif j==num_pat-1:\n",
        "                        out_concat[i*IMG_HEIGHT:i*IMG_HEIGHT+IMG_HEIGHT, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24] = sec1[i*num_pat+j]\n",
        "\n",
        "                elif i==num_pat-1:\n",
        "                    if j<num_pat-1:\n",
        "                        out_concat[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH:j*IMG_WIDTH+IMG_WIDTH] = sec1[i*num_pat+j]\n",
        "\n",
        "                    elif j==num_pat-1:\n",
        "                        out_concat[i*IMG_HEIGHT-24:i*IMG_HEIGHT+IMG_HEIGHT-24, j*IMG_WIDTH-24:j*IMG_WIDTH+IMG_WIDTH-24] = sec1[i*num_pat+j]\n",
        "\n",
        "        out_full[k] = out_concat\n",
        "\n",
        "    return out_full,test_ids\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    stain_norm_patch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "te391mbq3EZC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.backend import *\n",
        "\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "  ##### Metrices & Loss #####\n",
        "#------------- Metrice-------------\n",
        "def f1_score1(y_true,y_pred):\n",
        "    smooth=1\n",
        "    y_true_f = flatten(y_true)\n",
        "    y_pred_f = flatten(y_pred)\n",
        "    intersection = sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
        "#----------------------------------\n",
        "# -------------Loss----------------\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth=1\n",
        "    y_true_f = flatten(y_true)\n",
        "    y_pred_f = flatten(y_pred)\n",
        "    y_true_f = cast(y_true_f, dtype='float32')\n",
        "    y_pred_f = cast(y_pred_f, dtype='float32')\n",
        "    intersection = sum(y_true_f * y_pred_f)\n",
        "    return 1-(2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    def dice_coef_loss(y_true, y_pred):\n",
        "        return dice_loss(y_true, y_pred)\n",
        "\n",
        "    def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
        "        intersection = sum(abs(y_true * y_pred), axis=-1)\n",
        "        sum_ = sum(abs(y_true) + abs(y_pred), axis=-1)\n",
        "        jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "        return (1 - jac) * smooth\n",
        "\n",
        "    return (jaccard_distance_loss(y_true, y_pred) * dice_coef_loss(y_true, y_pred))/(jaccard_distance_loss(y_true, y_pred) + dice_coef_loss(y_true, y_pred))\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "#-----------------------------------\n",
        "#####################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "og_lr = 0.0001\n",
        "modified_lr = 0.0001"
      ],
      "metadata": {
        "id": "QT1kyv9Pmi-I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LOAz4jZL01MZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from  tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.activations import *\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.backend import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import *\n",
        "from loss_metric import loss,f1_score1,bce_dice_loss\n",
        "\n",
        "def create_model():\n",
        "\n",
        "    ## Crop and Merge Layers ##\n",
        "    def CropAndMerge(Input1, Input2):\n",
        "            \"\"\"\n",
        "            Crop input1 so that it matches input2 and then\n",
        "            return the concatenation of both channels.\n",
        "            \"\"\"\n",
        "            Size1_x = (Input1).shape[1]\n",
        "            Size2_x = (Input2).shape[1]\n",
        "\n",
        "            Size1_y = (Input1).shape[2]\n",
        "            Size2_y = (Input2).shape[2]\n",
        "\n",
        "            diff_x = tf.divide(tf.subtract(Size1_x, Size2_x), 2)\n",
        "            diff_y = tf.divide(tf.subtract(Size1_y, Size2_y), 2)\n",
        "            diff_x = tf.cast(diff_x, tf.int32)\n",
        "            Size2_x = tf.cast(Size2_x, tf.int32)\n",
        "            diff_y = tf.cast(diff_y, tf.int32)\n",
        "            Size2_y = tf.cast(Size2_y, tf.int32)\n",
        "            crop = tf.slice(Input1, [0, diff_x, diff_y, 0], [-1, Size2_x, Size2_y, -1])\n",
        "            concat = tf.concat([crop, Input2], axis=3)\n",
        "\n",
        "            return concat\n",
        "    #-------------------------------------\n",
        "    ## Attention Mechanism ##\n",
        "    def attention_gt(input_x,input_g, fil_las):\n",
        "        input_size = input_x.shape\n",
        "        fil_int = fil_las//2\n",
        "\n",
        "        input_g = Conv2D(filters=fil_las,kernel_size=(1,1), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "            kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_g)\n",
        "\n",
        "        theta_x = Conv2D(filters=fil_int,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                      kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_x)\n",
        "\n",
        "        theta_x_size  =  theta_x.shape\n",
        "\n",
        "        phi_g = Conv2D(filters=fil_int,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_g)\n",
        "\n",
        "        phi_g_u = UpSampling2D(size=(2, 2), interpolation='bilinear')(phi_g)\n",
        "        f = relu(add([theta_x,phi_g_u]))\n",
        "\n",
        "        # psi_f = Conv2D(filters=fil_las,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  # kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(f)\n",
        "\n",
        "\n",
        "        psi_f = Conv2D(filters=fil_las, kernel_size=(1,1), strides=(1, 1),activation='relu',padding='same')(f)\n",
        "        psi_f = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(psi_f)\n",
        "\n",
        "        sigm_psi_f = sigmoid(psi_f)\n",
        "\n",
        "        expand = Reshape(target_shape=input_size[-3:])(sigm_psi_f)\n",
        "\n",
        "\n",
        "        y = multiply([expand , input_x])\n",
        "\n",
        "        return y\n",
        "    ## Conv Block\n",
        "    def conv_block(input, filters):\n",
        "\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "    ## Bottleneck Block\n",
        "    def bottleneck_block(input, filters):\n",
        "\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    ## Robust Residual block\n",
        "    def robust_residual_block(input, filters_inp):\n",
        "\n",
        "        x1 = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1), padding='same',activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x1 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x1)\n",
        "\n",
        "        x2 = SeparableConv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1),activation='relu',padding='same',\n",
        "                            use_bias=True,depthwise_initializer='glorot_uniform',pointwise_initializer='glorot_uniform',\n",
        "                            kernel_initializer='glorot_normal',bias_initializer=Constant(0.1),depth_multiplier=1)(x1)\n",
        "        x2 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x2)\n",
        "\n",
        "        x3 = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1), padding='same',activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x2)\n",
        "        x3 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x3)\n",
        "\n",
        "        x = concatenate([input,x3],axis=-1)\n",
        "        x = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1),activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    # def skip_connection(input, filters_inp):\n",
        "    #     x1 = skip_connection_robust_residual(input, filters_inp)\n",
        "    #     x1 = skip_connection_robust\n",
        "\n",
        "    ## Attention Block ##\n",
        "    def attention_decoder_block(input, filt,conc):\n",
        "        atten_b = attention_gt(input_x=conc,input_g=input, fil_las=filt)\n",
        "        x_ct = Conv2DTranspose(filters=filt, kernel_size=(2, 2),activation='relu', strides=(2, 2), padding='same',kernel_initializer='glorot_normal',bias_initializer=Constant(0.1),use_bias=True)(input)\n",
        "        x = CropAndMerge(Input1=x_ct,Input2=atten_b)\n",
        "        return x\n",
        "\n",
        "    def nuclei_segnet(\n",
        "        input_shape,\n",
        "        num_classes=1,\n",
        "        output_activation='sigmoid'):\n",
        "\n",
        "        inputs = Input(input_shape)\n",
        "\n",
        "        filters = [32,64,128,256,512]\n",
        "\n",
        "\n",
        "        # for l in range(num_layers):\n",
        "        x_conv1 = robust_residual_block(inputs, filters[0])\n",
        "        x_pool1 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv1)\n",
        "        x_conv2 = robust_residual_block(x_pool1, filters[1])\n",
        "        x_pool2 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv2)\n",
        "        x_conv3 = robust_residual_block(x_pool2, filters[2])\n",
        "        x_pool3 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv3)\n",
        "        x_conv4 = robust_residual_block(x_pool3, filters[3])\n",
        "        x_pool4 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv4)\n",
        "        x_conv5 = bottleneck_block(x_pool4, filters[4])\n",
        "\n",
        "    # upsampling in the form of convtranspose\n",
        "\n",
        "        x_tconv5 = attention_decoder_block(x_conv5, filters[3],x_conv4)\n",
        "        u_conv4 = conv_block(x_tconv5, filters[3])\n",
        "        x_tconv4 = attention_decoder_block(u_conv4, filters[2],x_conv3)\n",
        "        u_conv3 = conv_block(x_tconv4, filters[2])\n",
        "        x_tconv3 = attention_decoder_block(u_conv3, filters[1],x_conv2)\n",
        "        u_conv2 = conv_block(x_tconv3, filters[1])\n",
        "        x_tconv2 = attention_decoder_block(u_conv2, filters[0],x_conv1)\n",
        "        u_conv1 = conv_block(x_tconv2, filters[0])\n",
        "\n",
        "        outputs = Conv2D(num_classes, kernel_size=(1,1), strides=(1,1), activation=output_activation, padding='same') (u_conv1)\n",
        "\n",
        "        model = Model(inputs=[inputs], outputs=[outputs])\n",
        "        return model\n",
        "\n",
        "    model = nuclei_segnet(\n",
        "                  input_shape=(256,256,3),\n",
        "                  num_classes=1,\n",
        "                  output_activation='sigmoid')\n",
        "\n",
        "    adam = optimizers.Adam(lr = og_lr)\n",
        "    model.compile(optimizer = adam,loss=loss,metrics=[f1_score1])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkY_Cod0PUr1",
        "outputId": "9db6f32b-9ac0-4e54-c7d3-d7839b9c3fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/611.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade keras"
      ],
      "metadata": {
        "id": "O2n7yFmay6gW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "\n",
        "input_shape = (256, 256, 3)\n",
        "image_size = input_shape[0]\n",
        "patch_size_t = 8  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size_t) ** 2\n",
        "projection_dim = 256\n",
        "num_heads = 6\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 3\n",
        "mlp_head_units = [\n",
        "    512,\n",
        "    256,\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.keras.activations.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size_t):\n",
        "        super().__init__()\n",
        "        self.patch_size_t = patch_size_t\n",
        "\n",
        "    def call(self, images):\n",
        "        input_shape = tf.shape(images)  # Direct shape access\n",
        "        batch_size = input_shape[0]\n",
        "        height = input_shape[1]\n",
        "        width = input_shape[2]\n",
        "        channels = input_shape[3]\n",
        "\n",
        "        num_patches_h = height // self.patch_size_t\n",
        "        num_patches_w = width // self.patch_size_t\n",
        "\n",
        "        patches = tf.image.extract_patches(\n",
        "            images,\n",
        "            sizes=[1, self.patch_size_t, self.patch_size_t, 1],\n",
        "            strides=[1, self.patch_size_t, self.patch_size_t, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "        print(patches)\n",
        "        patch_dims = patches.shape[-1]\n",
        "        # tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        patches = tf.reshape(\n",
        "            patches,\n",
        "            (batch_size, num_patches_h * num_patches_w, self.patch_size_t * self.patch_size_t * channels)\n",
        "        )\n",
        "\n",
        "        return patches\n",
        "\n",
        "    # def call(self, images):\n",
        "    #     input_shape = ops.shape(images)\n",
        "    #     batch_size = input_shape[0]\n",
        "    #     height = input_shape[1]\n",
        "    #     width = input_shape[2]\n",
        "    #     channels = input_shape[3]\n",
        "    #     num_patches_h = height // self.patch_size_t\n",
        "    #     num_patches_w = width // self.patch_size_t\n",
        "    #     patches = tf.keras.ops.image.extract_patches(images, size=self.patch_size_t)\n",
        "    #     # print(patches)\n",
        "    #     patches = ops.reshape(\n",
        "    #         patches,\n",
        "    #         (\n",
        "    #             batch_size,\n",
        "    #             num_patches_h * num_patches_w,\n",
        "    #             self.patch_size_t * self.patch_size_t * channels,\n",
        "    #         ),\n",
        "    #     )\n",
        "    #     return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size_t\": self.patch_size_t})\n",
        "        return config\n",
        "\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.expand_dims(\n",
        "            tf.range(start=0, limit=self.num_patches, delta=1), axis=0\n",
        "        )\n",
        "        projected_patches = self.projection(patch)\n",
        "        encoded = projected_patches + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches})\n",
        "        return config\n",
        "\n",
        "\n",
        "\n",
        "def vit(inputs):\n",
        "    # inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size_t)(inputs)\n",
        "    print(patches.shape)\n",
        "    # patches = layers.Conv2D(projection_dim, kernel_size=patch_size_t, strides=patch_size_t, padding='valid',name='patching')(inputs)\n",
        "    # patches = layers.Reshape((num_patches, projection_dim))(patches)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    # representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    # representation = layers.Flatten()(representation)\n",
        "    # representation = layers.Dropout(0.5)(representation)\n",
        "    # # # Add MLP.\n",
        "    # features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # # Classify outputs.\n",
        "    # logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    # model = tf.keras.Model(inputs=inputs, outputs=encoded_patches)\n",
        "    # model.summary()\n",
        "    return encoded_patches\n"
      ],
      "metadata": {
        "id": "F37DLcu4yHSg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vit()"
      ],
      "metadata": {
        "id": "BGvwwIvm_KhD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mk7-lBoi6mQE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from  tensorflow.keras.initializers import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.activations import *\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.backend import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import *\n",
        "from loss_metric import loss,f1_score1,bce_dice_loss\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "\n",
        "def create_model_modified():\n",
        "\n",
        "    ## Crop and Merge Layers ##\n",
        "    def CropAndMerge(Input1, Input2):\n",
        "            \"\"\"\n",
        "            Crop input1 so that it matches input2 and then\n",
        "            return the concatenation of both channels.\n",
        "            \"\"\"\n",
        "            Size1_x = (Input1).shape[1]\n",
        "            Size2_x = (Input2).shape[1]\n",
        "\n",
        "            Size1_y = (Input1).shape[2]\n",
        "            Size2_y = (Input2).shape[2]\n",
        "\n",
        "            diff_x = tf.divide(tf.subtract(Size1_x, Size2_x), 2)\n",
        "            diff_y = tf.divide(tf.subtract(Size1_y, Size2_y), 2)\n",
        "            diff_x = tf.cast(diff_x, tf.int32)\n",
        "            Size2_x = tf.cast(Size2_x, tf.int32)\n",
        "            diff_y = tf.cast(diff_y, tf.int32)\n",
        "            Size2_y = tf.cast(Size2_y, tf.int32)\n",
        "            crop = tf.slice(Input1, [0, diff_x, diff_y, 0], [-1, Size2_x, Size2_y, -1])\n",
        "            concat = tf.concat([crop, Input2], axis=3)\n",
        "\n",
        "            return concat\n",
        "    #-------------------------------------\n",
        "    ## Attention Mechanism ##\n",
        "    def attention_gt(input_x,input_g, fil_las):\n",
        "        input_size = input_x.shape\n",
        "        fil_int = fil_las//2\n",
        "\n",
        "        input_g = Conv2D(filters=fil_las,kernel_size=(1,1), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "            kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_g)\n",
        "\n",
        "        theta_x = Conv2D(filters=fil_int,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                      kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_x)\n",
        "\n",
        "        theta_x_size  =  theta_x.shape\n",
        "\n",
        "        phi_g = Conv2D(filters=fil_int,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input_g)\n",
        "\n",
        "        phi_g_u = UpSampling2D(size=(2, 2), interpolation='bilinear')(phi_g)\n",
        "        f = relu(add([theta_x,phi_g_u]))\n",
        "\n",
        "        # psi_f = Conv2D(filters=fil_las,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  # kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(f)\n",
        "\n",
        "\n",
        "        psi_f = Conv2D(filters=fil_las, kernel_size=(1,1), strides=(1, 1),activation='relu',padding='same')(f)\n",
        "        psi_f = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(psi_f)\n",
        "\n",
        "        sigm_psi_f = sigmoid(psi_f)\n",
        "\n",
        "        expand = Reshape(target_shape=input_size[-3:])(sigm_psi_f)\n",
        "\n",
        "\n",
        "        y = multiply([expand , input_x])\n",
        "\n",
        "        return y\n",
        "    ## Conv Block\n",
        "    def conv_block(input, filters):\n",
        "\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "    ## Bottleneck Block\n",
        "    def bottleneck_block(input, filters):\n",
        "\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "        x = Conv2D(filters,kernel_size=(3,3), strides=(1, 1), activation='relu', padding='same',use_bias=True,\n",
        "                  kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    ## Robust Residual block\n",
        "    def robust_residual_block(input, filters_inp):\n",
        "\n",
        "        x1 = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1), padding='same',activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(input)\n",
        "        x1 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x1)\n",
        "\n",
        "        x2 = SeparableConv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1),activation='relu',padding='same',\n",
        "                            use_bias=True,depthwise_initializer='glorot_uniform',pointwise_initializer='glorot_uniform',\n",
        "                            kernel_initializer='glorot_normal',bias_initializer=Constant(0.1),depth_multiplier=1)(x1)\n",
        "        x2 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x2)\n",
        "\n",
        "        x3 = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1), padding='same',activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x2)\n",
        "        x3 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                                gamma_initializer=Constant(1.0),momentum=0.5)(x3)\n",
        "\n",
        "        x = concatenate([input,x3],axis=-1)\n",
        "        x = Conv2D(filters=filters_inp,kernel_size=(3,3), strides=(1, 1),activation='relu', padding='same',use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(x)\n",
        "        x = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.01),\n",
        "                              gamma_initializer=Constant(1.0),momentum=0.5)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    # def skip_connection(input, filters_inp):\n",
        "    #     x1 = skip_connection_robust_residual(input, filters_inp)\n",
        "    #     x1 = skip_connection_robust\n",
        "\n",
        "    ## Attention Block ##\n",
        "    def attention_decoder_block(input, filt,conc):\n",
        "        atten_b = attention_gt(input_x=conc,input_g=input, fil_las=filt)\n",
        "        x_ct = Conv2DTranspose(filters=filt, kernel_size=(2, 2),activation='relu', strides=(2, 2), padding='same',kernel_initializer='glorot_normal',bias_initializer=Constant(0.1),use_bias=True)(input)\n",
        "        x = CropAndMerge(Input1=x_ct,Input2=atten_b)\n",
        "        return x\n",
        "\n",
        "    def fuse(w_g, w_x, channel):\n",
        "        w_add = add([w_g, w_x])\n",
        "        w_c_avg_pool = GlobalAveragePooling2D()(w_add)\n",
        "        b = mlp(w_c_avg_pool, [2*channel, channel], 0.5)\n",
        "        b = sigmoid(b)\n",
        "        b = Reshape(target_shape = (1, 1, channel))(b)\n",
        "        n_w_g = multiply([w_g, b])\n",
        "        n_w_x = multiply([w_x, b])\n",
        "        f = add([n_w_g, n_w_x])\n",
        "        return f\n",
        "\n",
        "    def Up(x_conv, filter):\n",
        "        upsampled = UpSampling2D(size=(2, 2), interpolation='bilinear')(x_conv)\n",
        "        conv1 = Conv2D(filter, kernel_size=(3, 3), padding='same', activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(upsampled)\n",
        "        bn1 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(conv1)\n",
        "        activation1 = Activation('relu')(bn1)\n",
        "\n",
        "        conv2 = Conv2D(filter, kernel_size=(3, 3), padding='same', activation='relu',\n",
        "                    use_bias=True, kernel_initializer='glorot_normal',bias_initializer=Constant(0.1))(activation1)\n",
        "        bn2 = BatchNormalization(epsilon=1e-3,beta_initializer=Constant(0.0),gamma_initializer=Constant(1.0),momentum=0.5)(conv2)\n",
        "        activation2 = Activation('relu')(bn2)\n",
        "\n",
        "        return activation2\n",
        "\n",
        "    def nuclei_segnet(\n",
        "        input_shape,\n",
        "        num_classes=1,\n",
        "        output_activation='sigmoid'):\n",
        "\n",
        "        image_size = input_shape[0]\n",
        "        inputs = Input(input_shape)\n",
        "\n",
        "        filters = [32,64,128,256,512]\n",
        "\n",
        "\n",
        "        # for l in range(num_layers):\n",
        "        g_conv4 = vit(inputs)\n",
        "        g_conv4 = layers.Reshape(target_shape = (int(image_size/8), int(image_size/8), filters[3]))(g_conv4)\n",
        "        print(g_conv4.shape)\n",
        "\n",
        "        x_conv1 = robust_residual_block(inputs, filters[0])\n",
        "        x_pool1 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv1)\n",
        "        x_conv2 = robust_residual_block(x_pool1, filters[1])\n",
        "        x_pool2 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv2)\n",
        "        x_conv3 = robust_residual_block(x_pool2, filters[2])\n",
        "        x_pool3 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv3)\n",
        "        x_conv4 = robust_residual_block(x_pool3, filters[3])\n",
        "        x_pool4 = MaxPooling2D((2, 2), strides=(2, 2),padding=\"same\")(x_conv4)\n",
        "        x_conv5 = bottleneck_block(x_pool4, filters[4])\n",
        "\n",
        "        x_g_conv4 = fuse(g_conv4, x_conv4, filters[3])\n",
        "\n",
        "        g_conv3 = Up(g_conv4, filters[2])\n",
        "        x_g_conv3 = fuse(g_conv3, x_conv3, filters[2])\n",
        "\n",
        "        g_conv2 = Up(g_conv3, filters[1])\n",
        "        x_g_conv2 = fuse(g_conv2, x_conv2, filters[1])\n",
        "\n",
        "        g_conv1 = Up(g_conv2, filters[0])\n",
        "        x_g_conv1 = fuse(g_conv1, x_conv1, filters[0])\n",
        "\n",
        "\n",
        "    # upsampling in the form of convtranspose\n",
        "\n",
        "        x_tconv5 = attention_decoder_block(x_conv5, filters[3],x_g_conv4)\n",
        "        u_conv4 = conv_block(x_tconv5, filters[3])\n",
        "        x_tconv4 = attention_decoder_block(u_conv4, filters[2],x_g_conv3)\n",
        "        u_conv3 = conv_block(x_tconv4, filters[2])\n",
        "        x_tconv3 = attention_decoder_block(u_conv3, filters[1],x_g_conv2)\n",
        "        u_conv2 = conv_block(x_tconv3, filters[1])\n",
        "        x_tconv2 = attention_decoder_block(u_conv2, filters[0],x_g_conv1)\n",
        "        u_conv1 = conv_block(x_tconv2, filters[0])\n",
        "\n",
        "        outputs = Conv2D(num_classes, kernel_size=(1,1), strides=(1,1), activation=output_activation, padding='same') (u_conv1)\n",
        "\n",
        "        model = Model(inputs=[inputs], outputs=[outputs])\n",
        "        return model\n",
        "\n",
        "    model = nuclei_segnet(\n",
        "                  input_shape=(256,256,3),\n",
        "                  num_classes=1,\n",
        "                  output_activation='sigmoid')\n",
        "    model.summary()\n",
        "    adam = optimizers.Adam(lr = modified_lr)\n",
        "    model.compile(optimizer = adam,loss=loss,metrics=[f1_score1])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rVWsFf6YGaCS"
      },
      "outputs": [],
      "source": [
        "from skimage.io import imread\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "#%matplotlib inline\n",
        "from albumentations import (Blur, Compose, HorizontalFlip, HueSaturationValue,\n",
        "                            IAAEmboss, IAASharpen, JpegCompression, OneOf,\n",
        "                            RandomBrightness, RandomBrightnessContrast,\n",
        "                            RandomContrast, RandomCrop, RandomGamma,\n",
        "                            RandomRotate90, RGBShift, ShiftScaleRotate,\n",
        "                            Transpose, VerticalFlip, ElasticTransform, GridDistortion, OpticalDistortion)\n",
        "\n",
        "import albumentations as albu\n",
        "from albumentations import Resize, Crop\n",
        "# from  albumentations.augmentations.transforms import GaussianBlur\n",
        "\n",
        "def aug_with_crop(image_size = 256, crop_prob = 1):\n",
        "    return Compose([\n",
        "        RandomCrop(width = image_size, height = image_size, p=crop_prob),\n",
        "        HorizontalFlip(p=0.5),\n",
        "        VerticalFlip(p=0.5),\n",
        "        RandomRotate90(p=0.5),\n",
        "        Transpose(p=0.5),\n",
        "        # ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
        "        RandomBrightnessContrast(p=0.5),\n",
        "        # RandomGamma(p=0.25),\n",
        "        # IAAEmboss(p=0.25),\n",
        "        Blur(p=0.3, blur_limit = 3),\n",
        "        # GaussianBlur(p=0.5, blur_limit = 3),\n",
        "        # OneOf([\n",
        "        #     ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "        #     GridDistortion(p=0.5),\n",
        "        #     OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)\n",
        "        # ], p=0.5)\n",
        "    ], p = 1)\n",
        "\n",
        "\n",
        "class DataGeneratorFolder(Sequence):\n",
        "    def __init__(self, root_dir=r'/data/val_test', image_folder='tis/', mask_folder='Bin/',\n",
        "                 batch_size=1, image_size=256, nb_y_features=1,\n",
        "                 augmentation=None,\n",
        "                 suffle=True):\n",
        "        self.image_filenames = sorted(os.listdir(os.path.join(root_dir, image_folder)))\n",
        "        self.mask_names = sorted(os.listdir(os.path.join(root_dir, mask_folder)))\n",
        "        self.batch_size = batch_size\n",
        "        self.currentIndex = 0\n",
        "        self.augmentation = augmentation\n",
        "        self.image_size = image_size\n",
        "        self.nb_y_features = nb_y_features\n",
        "        self.indexes = None\n",
        "        self.suffle = suffle\n",
        "        self.path1= root_dir\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Calculates size of batch\n",
        "        \"\"\"\n",
        "        return int(np.ceil(len(self.image_filenames) / (self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.suffle==True:\n",
        "            self.image_filenames, self.mask_names = shuffle(self.image_filenames, self.mask_names)\n",
        "\n",
        "\n",
        "    def read_image_mask(self, image_name, mask_name,path1):\n",
        "        i_path=path1+\"/tis/\"\n",
        "        m_path = path1+\"/Bin/\"\n",
        "\n",
        "        img1 = cv2.imread(i_path+image_name)\n",
        "        img2 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        img2 = img2/255\n",
        "        mask1 = cv2.imread(m_path+mask_name,0)\n",
        "        mask1 = mask1.astype(np.uint8)\n",
        "\n",
        "        return img2, mask1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate indexes of the batch\n",
        "        data_index_min = int(index*self.batch_size)\n",
        "\n",
        "        # data_index_max = int(min((index+1)*self.batch_size, len(self.image_filenames)))\n",
        "        data_index_max = int((index+1)*self.batch_size)\n",
        "        indexes = self.image_filenames[data_index_min:data_index_max]\n",
        "\n",
        "        this_batch_size = len(indexes) # The last batch can be smaller than the others\n",
        "\n",
        "        # Defining dataset\n",
        "        X = np.zeros((this_batch_size, self.image_size, self.image_size, 3), dtype=np.float32)\n",
        "        y = np.zeros((this_batch_size, self.image_size, self.image_size, self.nb_y_features), dtype=bool)\n",
        "\n",
        "        for i, sample_index in enumerate(indexes):\n",
        "            # print(sample_index)\n",
        "            # print(index)\n",
        "            X_sample, y_sample = self.read_image_mask(self.image_filenames[index * self.batch_size + i],\n",
        "                                                    self.mask_names[index * self.batch_size + i],self.path1)\n",
        "\n",
        "            # if augmentation is defined, we assume its a train set\n",
        "            if self.augmentation is not None:\n",
        "\n",
        "                # Augmentation code\n",
        "                augmented = self.augmentation(self.image_size)(image=X_sample.astype(np.float32), mask=y_sample.astype(np.uint8))\n",
        "                image_augm = augmented['image']\n",
        "                mask_augm = augmented['mask'].reshape(self.image_size, self.image_size, self.nb_y_features)\n",
        "                X[i, ...] = np.clip(image_augm, a_min = 0, a_max=1).astype(np.float32)\n",
        "                y[i, ...] = mask_augm.astype(bool)\n",
        "\n",
        "            # if augmentation isnt defined, we assume its a test set.\n",
        "            # Because test images can have different sizes we resize it to be divisable by 32\n",
        "            elif self.augmentation is None and self.batch_size ==1:\n",
        "                X_sample, y_sample = self.read_image_mask(self.image_filenames[index * 1 + i],\n",
        "                                                      self.mask_names[index * 1 + i],self.path1)\n",
        "                # augmented = Resize(height=(X_sample.shape[0]//32)*32, width=(X_sample.shape[1]//32)*32)(image = X_sample, mask = y_sample)\n",
        "                augmented = RandomCrop(width = self.image_size, height = self.image_size, p=1)(image = X_sample, mask = y_sample)\n",
        "                X_sample, y_sample = augmented['image'], augmented['mask']\n",
        "\n",
        "                return X_sample.reshape(1, X_sample.shape[0], X_sample.shape[1], 3).astype(np.float32),\\\n",
        "                       y_sample.reshape(1, X_sample.shape[0], X_sample.shape[1], self.nb_y_features).astype(bool)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3fEQ9i6-yY",
        "outputId": "fc012412-f1cd-46e4-fb53-d6baa4d7a8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"patches/ExtractImagePatches:0\", shape=(None, 32, 32, 192), dtype=float32)\n",
            "(None, 1024, 192)\n",
            "(None, 32, 32, 256)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 256, 256, 32)         896       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 256, 256, 32)         128       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " separable_conv2d (Separabl  (None, 256, 256, 32)         1344      ['batch_normalization[0][0]'] \n",
            " eConv2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 256, 256, 32)         128       ['separable_conv2d[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 32)         9248      ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 256, 256, 32)         128       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)      (None, 256, 256, 35)         0         ['input_1[0][0]',             \n",
            "                                                                     'batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 256, 256, 32)         10112     ['tf.concat[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 256, 256, 32)         128       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 32)         0         ['batch_normalization_3[0][0]'\n",
            " D)                                                                 ]                             \n",
            "                                                                                                  \n",
            " patches (Patches)           (None, 1024, 192)            0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 64)         18496     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncode  (None, 1024, 256)            311552    ['patches[0][0]']             \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 128, 128, 64)         256       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 1024, 256)            512       ['patch_encoder[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (Separa  (None, 128, 128, 64)         4736      ['batch_normalization_4[0][0]'\n",
            " bleConv2D)                                                         ]                             \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 1024, 256)            1577728   ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 128, 128, 64)         256       ['separable_conv2d_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 1024, 256)            0         ['multi_head_attention[0][0]',\n",
            "                                                                     'patch_encoder[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 128, 128, 64)         36928     ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 1024, 256)            512       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 128, 128, 64)         256       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1024, 512)            131584    ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)    (None, 128, 128, 96)         0         ['max_pooling2d[0][0]',       \n",
            "                                                                     'batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 1024, 512)            0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 128, 128, 64)         55360     ['tf.concat_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1024, 256)            131328    ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 128, 128, 64)         256       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 1024, 256)            0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 64)           0         ['batch_normalization_7[0][0]'\n",
            " g2D)                                                               ]                             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 1024, 256)            0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 64, 64, 128)          73856     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 1024, 256)            512       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 64, 64, 128)          512       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 1024, 256)            1577728   ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (Separa  (None, 64, 64, 128)          17664     ['batch_normalization_8[0][0]'\n",
            " bleConv2D)                                                         ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 1024, 256)            0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'add_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 64, 64, 128)          512       ['separable_conv2d_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 1024, 256)            512       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 64, 64, 128)          147584    ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1024, 512)            131584    ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 64, 64, 128)          512       ['conv2d_7[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 1024, 512)            0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " tf.concat_2 (TFOpLambda)    (None, 64, 64, 192)          0         ['max_pooling2d_1[0][0]',     \n",
            "                                                                     'batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 1024, 256)            131328    ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 128)          221312    ['tf.concat_2[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 1024, 256)            0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 64, 64, 128)          512       ['conv2d_8[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 1024, 256)            0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 128)          0         ['batch_normalization_11[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 1024, 256)            512       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 256)          295168    ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 1024, 256)            1577728   ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 32, 32, 256)          1024      ['conv2d_9[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 1024, 256)            0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'add_3[0][0]']               \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (Separa  (None, 32, 32, 256)          68096     ['batch_normalization_12[0][0]\n",
            " bleConv2D)                                                         ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 1024, 256)            512       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 32, 32, 256)          1024      ['separable_conv2d_3[0][0]']  \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1024, 512)            131584    ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 1024, 512)            0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 32, 32, 256)          1024      ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 1024, 256)            131328    ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " tf.concat_3 (TFOpLambda)    (None, 32, 32, 384)          0         ['max_pooling2d_2[0][0]',     \n",
            "                                                                     'batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 1024, 256)            0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 256)          884992    ['tf.concat_3[0][0]']         \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 1024, 256)            0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 32, 32, 256)          1024      ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 32, 32, 256)          0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 32, 32, 256)          0         ['reshape[0][0]',             \n",
            "                                                                     'batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 256)          0         ['batch_normalization_15[0][0]\n",
            " g2D)                                                               ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 256)                  0         ['add_6[0][0]']               \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 512)          1180160   ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 512)                  131584    ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 16, 16, 512)          2048      ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 512)                  0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 256)                  131328    ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 16, 16, 512)          2048      ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 256)                  0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 16, 16, 512)          2359808   ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid (TFOpLambd  (None, 256)                  0         ['dropout_7[0][0]']           \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 16, 16, 512)          2048      ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 1, 1, 256)            0         ['tf.math.sigmoid[0][0]']     \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 32, 32, 256)          0         ['reshape[0][0]',             \n",
            "                                                                     'reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)       (None, 32, 32, 256)          0         ['batch_normalization_15[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 16, 16, 256)          131328    ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2  (None, 64, 64, 256)          0         ['reshape[0][0]']             \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 32, 32, 256)          0         ['multiply[0][0]',            \n",
            "                                                                     'multiply_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 16, 16, 128)          295040    ['conv2d_21[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 64, 64, 128)          295040    ['up_sampling2d[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 32, 32, 128)          295040    ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSamplin  (None, 32, 32, 128)          0         ['conv2d_23[0][0]']           \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 64, 64, 128)          512       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 32, 32, 128)          0         ['conv2d_22[0][0]',           \n",
            "                                                                     'up_sampling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 64, 64, 128)          0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.nn.relu (TFOpLambda)     (None, 32, 32, 128)          0         ['add_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 64, 64, 128)          147584    ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 32, 32, 256)          33024     ['tf.nn.relu[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 64, 64, 128)          512       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 32, 32, 256)          1024      ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 64, 64, 128)          0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_4 (TFOpLam  (None, 32, 32, 256)          0         ['batch_normalization_25[0][0]\n",
            " bda)                                                               ']                            \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 64, 64, 128)          0         ['activation_1[0][0]',        \n",
            "                                                                     'batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 32, 32, 256)          524544    ['batch_normalization_18[0][0]\n",
            " anspose)                                                           ']                            \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)         (None, 32, 32, 256)          0         ['tf.math.sigmoid_4[0][0]']   \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 128)                  0         ['add_8[0][0]']               \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " tf.slice (TFOpLambda)       (None, 32, 32, 256)          0         ['conv2d_transpose[0][0]']    \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)       (None, 32, 32, 256)          0         ['reshape_5[0][0]',           \n",
            "                                                                     'add_7[0][0]']               \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 256)                  33024     ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)    (None, 32, 32, 512)          0         ['tf.slice[0][0]',            \n",
            "                                                                     'multiply_8[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 256)                  0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 32, 32, 256)          1179904   ['tf.concat_4[0][0]']         \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 128)                  32896     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 32, 32, 256)          1024      ['conv2d_25[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 128)                  0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 32, 32, 256)          590080    ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_1 (TFOpLam  (None, 128)                  0         ['dropout_9[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 32, 32, 256)          1024      ['conv2d_26[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 1, 1, 128)            0         ['tf.math.sigmoid_1[0][0]']   \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)       (None, 64, 64, 128)          0         ['activation_1[0][0]',        \n",
            "                                                                     'reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)       (None, 64, 64, 128)          0         ['batch_normalization_11[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 32, 32, 128)          32896     ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSamplin  (None, 128, 128, 128)        0         ['activation_1[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 64, 64, 128)          0         ['multiply_2[0][0]',          \n",
            "                                                                     'multiply_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 32, 32, 64)           73792     ['conv2d_27[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 64)         73792     ['up_sampling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 64, 64, 64)           73792     ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSamplin  (None, 64, 64, 64)           0         ['conv2d_29[0][0]']           \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 128, 128, 64)         256       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 64, 64, 64)           0         ['conv2d_28[0][0]',           \n",
            "                                                                     'up_sampling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 128, 128, 64)         0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 64, 64, 64)           0         ['add_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 128, 128, 64)         36928     ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)          (None, 64, 64, 128)          8320      ['tf.nn.relu_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 128, 128, 64)         256       ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 64, 64, 128)          512       ['conv2d_30[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 128, 128, 64)         0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_5 (TFOpLam  (None, 64, 64, 128)          0         ['batch_normalization_28[0][0]\n",
            " bda)                                                               ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 128, 128, 64)         0         ['activation_3[0][0]',        \n",
            "                                                                     'batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 64, 64, 128)          131200    ['batch_normalization_27[0][0]\n",
            " Transpose)                                                         ']                            \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)         (None, 64, 64, 128)          0         ['tf.math.sigmoid_5[0][0]']   \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2  (None, 64)                   0         ['add_10[0][0]']              \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " tf.slice_1 (TFOpLambda)     (None, 64, 64, 128)          0         ['conv2d_transpose_1[0][0]']  \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)       (None, 64, 64, 128)          0         ['reshape_6[0][0]',           \n",
            "                                                                     'add_9[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 128)                  8320      ['global_average_pooling2d_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)    (None, 64, 64, 256)          0         ['tf.slice_1[0][0]',          \n",
            "                                                                     'multiply_9[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 128)                  0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)          (None, 64, 64, 128)          295040    ['tf.concat_5[0][0]']         \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 64)                   8256      ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 64, 64, 128)          512       ['conv2d_31[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 64)                   0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)          (None, 64, 64, 128)          147584    ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_2 (TFOpLam  (None, 64)                   0         ['dropout_11[0][0]']          \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_30 (Ba  (None, 64, 64, 128)          512       ['conv2d_32[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 1, 1, 64)             0         ['tf.math.sigmoid_2[0][0]']   \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)       (None, 128, 128, 64)         0         ['activation_3[0][0]',        \n",
            "                                                                     'reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)       (None, 128, 128, 64)         0         ['batch_normalization_7[0][0]'\n",
            "                                                                    , 'reshape_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)          (None, 64, 64, 64)           8256      ['batch_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSamplin  (None, 256, 256, 64)         0         ['activation_3[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 128, 128, 64)         0         ['multiply_4[0][0]',          \n",
            "                                                                     'multiply_5[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)          (None, 64, 64, 32)           18464     ['conv2d_33[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 256, 256, 32)         18464     ['up_sampling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)          (None, 128, 128, 32)         18464     ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSamplin  (None, 128, 128, 32)         0         ['conv2d_35[0][0]']           \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 256, 256, 32)         128       ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 128, 128, 32)         0         ['conv2d_34[0][0]',           \n",
            "                                                                     'up_sampling2d_5[0][0]']     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 256, 256, 32)         0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 128, 128, 32)         0         ['add_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 256, 256, 32)         9248      ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)          (None, 128, 128, 64)         2112      ['tf.nn.relu_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 256, 256, 32)         128       ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_31 (Ba  (None, 128, 128, 64)         256       ['conv2d_36[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 256, 256, 32)         0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_6 (TFOpLam  (None, 128, 128, 64)         0         ['batch_normalization_31[0][0]\n",
            " bda)                                                               ']                            \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 256, 256, 32)         0         ['activation_5[0][0]',        \n",
            "                                                                     'batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 64)         32832     ['batch_normalization_30[0][0]\n",
            " Transpose)                                                         ']                            \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)         (None, 128, 128, 64)         0         ['tf.math.sigmoid_6[0][0]']   \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3  (None, 32)                   0         ['add_12[0][0]']              \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " tf.slice_2 (TFOpLambda)     (None, 128, 128, 64)         0         ['conv2d_transpose_2[0][0]']  \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)      (None, 128, 128, 64)         0         ['reshape_7[0][0]',           \n",
            "                                                                     'add_11[0][0]']              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 64)                   2112      ['global_average_pooling2d_3[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " tf.concat_6 (TFOpLambda)    (None, 128, 128, 128)        0         ['tf.slice_2[0][0]',          \n",
            "                                                                     'multiply_10[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 64)                   0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)          (None, 128, 128, 64)         73792     ['tf.concat_6[0][0]']         \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 32)                   2080      ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (Ba  (None, 128, 128, 64)         256       ['conv2d_37[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 32)                   0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)          (None, 128, 128, 64)         36928     ['batch_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_3 (TFOpLam  (None, 32)                   0         ['dropout_13[0][0]']          \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_33 (Ba  (None, 128, 128, 64)         256       ['conv2d_38[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)         (None, 1, 1, 32)             0         ['tf.math.sigmoid_3[0][0]']   \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)       (None, 256, 256, 32)         0         ['activation_5[0][0]',        \n",
            "                                                                     'reshape_4[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)       (None, 256, 256, 32)         0         ['batch_normalization_3[0][0]'\n",
            "                                                                    , 'reshape_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)          (None, 128, 128, 32)         2080      ['batch_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 256, 256, 32)         0         ['multiply_6[0][0]',          \n",
            "                                                                     'multiply_7[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)          (None, 128, 128, 16)         4624      ['conv2d_39[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)          (None, 256, 256, 16)         4624      ['add_13[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSamplin  (None, 256, 256, 16)         0         ['conv2d_41[0][0]']           \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 256, 256, 16)         0         ['conv2d_40[0][0]',           \n",
            "                                                                     'up_sampling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)   (None, 256, 256, 16)         0         ['add_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)          (None, 256, 256, 32)         544       ['tf.nn.relu_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_34 (Ba  (None, 256, 256, 32)         128       ['conv2d_42[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_7 (TFOpLam  (None, 256, 256, 32)         0         ['batch_normalization_34[0][0]\n",
            " bda)                                                               ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 32)         8224      ['batch_normalization_33[0][0]\n",
            " Transpose)                                                         ']                            \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)         (None, 256, 256, 32)         0         ['tf.math.sigmoid_7[0][0]']   \n",
            "                                                                                                  \n",
            " tf.slice_3 (TFOpLambda)     (None, 256, 256, 32)         0         ['conv2d_transpose_3[0][0]']  \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)      (None, 256, 256, 32)         0         ['reshape_8[0][0]',           \n",
            "                                                                     'add_13[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat_7 (TFOpLambda)    (None, 256, 256, 64)         0         ['tf.slice_3[0][0]',          \n",
            "                                                                     'multiply_11[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)          (None, 256, 256, 32)         18464     ['tf.concat_7[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_35 (Ba  (None, 256, 256, 32)         128       ['conv2d_43[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)          (None, 256, 256, 32)         9248      ['batch_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_36 (Ba  (None, 256, 256, 32)         128       ['conv2d_44[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)          (None, 256, 256, 1)          33        ['batch_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19174497 (73.14 MB)\n",
            "Trainable params: 19163809 (73.10 MB)\n",
            "Non-trainable params: 10688 (41.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "Tensor(\"model/patches/ExtractImagePatches:0\", shape=(None, None, None, None), dtype=float32)\n",
            "Tensor(\"model/patches/ExtractImagePatches:0\", shape=(None, None, None, None), dtype=float32)\n",
            "148/148 [==============================] - ETA: 0s - loss: 0.0384 - f1_score1: 0.7197Tensor(\"model/patches/ExtractImagePatches:0\", shape=(None, None, None, None), dtype=float32)\n",
            "148/148 [==============================] - 157s 474ms/step - loss: 0.0384 - f1_score1: 0.7197 - val_loss: 0.0280 - val_f1_score1: 0.7488\n",
            "Epoch 2/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0248 - f1_score1: 0.7609 - val_loss: 0.0264 - val_f1_score1: 0.7256\n",
            "Epoch 3/40\n",
            "148/148 [==============================] - 67s 449ms/step - loss: 0.0201 - f1_score1: 0.7843 - val_loss: 0.0218 - val_f1_score1: 0.7606\n",
            "Epoch 4/40\n",
            "148/148 [==============================] - 67s 453ms/step - loss: 0.0181 - f1_score1: 0.7953 - val_loss: 0.0229 - val_f1_score1: 0.7617\n",
            "Epoch 5/40\n",
            "148/148 [==============================] - 67s 447ms/step - loss: 0.0170 - f1_score1: 0.8012 - val_loss: 0.0227 - val_f1_score1: 0.7548\n",
            "Epoch 6/40\n",
            "148/148 [==============================] - 67s 453ms/step - loss: 0.0166 - f1_score1: 0.8036 - val_loss: 0.0194 - val_f1_score1: 0.7766\n",
            "Epoch 7/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0165 - f1_score1: 0.8032 - val_loss: 0.0193 - val_f1_score1: 0.7833\n",
            "Epoch 8/40\n",
            "148/148 [==============================] - 67s 447ms/step - loss: 0.0166 - f1_score1: 0.8026 - val_loss: 0.0220 - val_f1_score1: 0.7563\n",
            "Epoch 9/40\n",
            "148/148 [==============================] - 67s 453ms/step - loss: 0.0155 - f1_score1: 0.8100 - val_loss: 0.0168 - val_f1_score1: 0.7917\n",
            "Epoch 10/40\n",
            "148/148 [==============================] - 67s 451ms/step - loss: 0.0156 - f1_score1: 0.8087 - val_loss: 0.0183 - val_f1_score1: 0.7813\n",
            "Epoch 11/40\n",
            "148/148 [==============================] - 67s 452ms/step - loss: 0.0153 - f1_score1: 0.8107 - val_loss: 0.0177 - val_f1_score1: 0.7820\n",
            "Epoch 12/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0151 - f1_score1: 0.8125 - val_loss: 0.0251 - val_f1_score1: 0.7557\n",
            "Epoch 13/40\n",
            "148/148 [==============================] - 67s 449ms/step - loss: 0.0153 - f1_score1: 0.8099 - val_loss: 0.0224 - val_f1_score1: 0.7427\n",
            "Epoch 14/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0152 - f1_score1: 0.8127 - val_loss: 0.0187 - val_f1_score1: 0.7755\n",
            "Epoch 15/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0149 - f1_score1: 0.8134 - val_loss: 0.0189 - val_f1_score1: 0.7703\n",
            "Epoch 16/40\n",
            "148/148 [==============================] - 68s 455ms/step - loss: 0.0147 - f1_score1: 0.8149 - val_loss: 0.0194 - val_f1_score1: 0.7759\n",
            "Epoch 17/40\n",
            "148/148 [==============================] - 67s 449ms/step - loss: 0.0151 - f1_score1: 0.8124 - val_loss: 0.0185 - val_f1_score1: 0.7767\n",
            "Epoch 18/40\n",
            "148/148 [==============================] - 67s 450ms/step - loss: 0.0147 - f1_score1: 0.8153 - val_loss: 0.0188 - val_f1_score1: 0.7819\n",
            "Epoch 19/40\n",
            "148/148 [==============================] - 67s 447ms/step - loss: 0.0144 - f1_score1: 0.8162 - val_loss: 0.0174 - val_f1_score1: 0.7849\n",
            "Epoch 20/40\n",
            "148/148 [==============================] - 67s 447ms/step - loss: 0.0143 - f1_score1: 0.8166 - val_loss: 0.0202 - val_f1_score1: 0.7612\n",
            "Epoch 21/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0140 - f1_score1: 0.8189 - val_loss: 0.0175 - val_f1_score1: 0.7828\n",
            "Epoch 22/40\n",
            "148/148 [==============================] - 67s 452ms/step - loss: 0.0145 - f1_score1: 0.8155 - val_loss: 0.0175 - val_f1_score1: 0.7927\n",
            "Epoch 23/40\n",
            "148/148 [==============================] - 67s 452ms/step - loss: 0.0140 - f1_score1: 0.8191 - val_loss: 0.0195 - val_f1_score1: 0.7677\n",
            "Epoch 24/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0142 - f1_score1: 0.8178 - val_loss: 0.0189 - val_f1_score1: 0.7772\n",
            "Epoch 25/40\n",
            "148/148 [==============================] - 67s 448ms/step - loss: 0.0140 - f1_score1: 0.8194 - val_loss: 0.0180 - val_f1_score1: 0.7796\n",
            "Epoch 26/40\n",
            "119/148 [=======================>......] - ETA: 11s - loss: 0.0140 - f1_score1: 0.8186"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import tensorflow\n",
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "\n",
        "\n",
        "# Path to the stain normalized patch\n",
        "valid_data_path = '/content/ValidData/'\n",
        "train_data_path = '/content/TrainData/'\n",
        "\n",
        "# path for the weight and the saved history scores( loss and metrices) over the epochs\n",
        "if not os.path.exists(\"/content/checkpoint\"):\n",
        "    os.mkdir(\"/content/checkpoint\")\n",
        "\n",
        "if not os.path.exists(\"/content/history\"):\n",
        "    os.mkdir(\"/content/history\")\n",
        "\n",
        "\n",
        "model_path = '/content/checkpoint'\n",
        "weight_name1 = 'nuclei_seg_og.h5'  # name of the weight for the final model\n",
        "weight_name2 = 'nuclei_seg_modified.h5'\n",
        "history_path = '/content/history'\n",
        "hist_name1 = 'nuclei_seg_og.csv'  # name of the csv file for storing all scores\n",
        "hist_name2 = 'nuclei_seg_modified.csv'\n",
        "\n",
        "patch_size = 256\n",
        "bs = 4 # batch size for training for validation it is taken 1\n",
        "eps = 40 # epochs\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "val_generator = DataGeneratorFolder(root_dir = valid_data_path,\n",
        "                                    image_folder = 'tis/',\n",
        "                                    mask_folder = 'Bin/',\n",
        "                                    batch_size=1,augmentation = None,\n",
        "                                    image_size=patch_size,\n",
        "                                    nb_y_features = 1)\n",
        "\n",
        "train_generator = DataGeneratorFolder(root_dir = train_data_path,\n",
        "                                      image_folder = 'tis/',\n",
        "                                      mask_folder = 'Bin/',\n",
        "                                      augmentation = aug_with_crop,\n",
        "                                      batch_size=bs,\n",
        "                                      image_size=patch_size,\n",
        "                                      nb_y_features = 1)\n",
        "\n",
        "\n",
        "# reduces learning rate on plateau\n",
        "lr_reducer = ReduceLROnPlateau(factor=0.1,patience=5,\n",
        "                               cooldown= 5,\n",
        "                               min_lr=0.1e-5,verbose=1)\n",
        "# # model autosave callbacks\n",
        "# mode_autosave = ModelCheckpoint(\"kidney_mod_kumar.h5\",\n",
        "#                                 monitor='val_f1-score',\n",
        "#                                 mode='max', save_best_only=True, verbose=1, save_freq=65)\n",
        "\n",
        "# # stop learining as metric on validatopn stop increasing\n",
        "# early_stopping = EarlyStopping(patience=5, verbose=1, mode = 'auto')\n",
        "\n",
        "# # tensorboard for monitoring logs\n",
        "# tensorboard = TensorBoard(log_dir='./logs/tenboard', histogram_freq=0,\n",
        "#                           write_graph=True, write_images=False)\n",
        "\n",
        "cbks = []\n",
        "\n",
        "\n",
        "model2 = create_model_modified()\n",
        "history2 = model2.fit(train_generator, shuffle =True,\n",
        "                  epochs=eps, workers=4, use_multiprocessing=True,\n",
        "                  validation_data = val_generator,\n",
        "                  verbose = 1,callbacks=cbks)\n",
        "\n",
        "model1 = create_model()\n",
        "history1 = model1.fit(train_generator, shuffle =True,\n",
        "                  epochs=eps, workers=4, use_multiprocessing=True,\n",
        "                  validation_data = val_generator,\n",
        "                  verbose = 1,callbacks=cbks)\n",
        "\n",
        "\n",
        "print('Training complete =========>')\n",
        "# saving model last weight and the history of the scores\n",
        "print('Writing the model weights and the history =========>')\n",
        "model1.save(os.path.join(model_path,weight_name1))\n",
        "model2.save(os.path.join(model_path,weight_name2))\n",
        "hist_df1 = pd.DataFrame(history1.history)\n",
        "hist_csv_file = os.path.join(history_path,hist_name1)\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df1.to_csv(f)\n",
        "\n",
        "hist_df2 = pd.DataFrame(history2.history)\n",
        "hist_csv_file = os.path.join(history_path,hist_name2)\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df2.to_csv(f)\n",
        "print('Save complete =========>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-N89RdxYg1T"
      },
      "outputs": [],
      "source": [
        "loss1 = history1.history['loss']\n",
        "loss2 = history2.history['loss']\n",
        "\n",
        "# Plotting the loss values\n",
        "epochs = range(1, len(loss1) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting model1 loss\n",
        "plt.plot(epochs, loss1, 'b-', label='Training Loss - Existing')\n",
        "\n",
        "# Plotting model2 loss\n",
        "plt.plot(epochs, loss2, 'r-', label='Training Loss - Proposed')\n",
        "\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLJBK8TX8ABE"
      },
      "outputs": [],
      "source": [
        "val_loss1 = history1.history['val_loss']\n",
        "val_loss2 = history2.history['val_loss']\n",
        "\n",
        "# Plotting the validation loss values\n",
        "epochs = range(1, len(val_loss1) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting model1 validation loss\n",
        "plt.plot(epochs, val_loss1, 'b-', label='Validation Loss - Existing')\n",
        "\n",
        "# Plotting model2 validation loss\n",
        "plt.plot(epochs, val_loss2, 'r-', label='Validation Loss - Proposed')\n",
        "\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx9dzT1cztop"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def iou_metric(y_true_in, y_pred_in):\n",
        "    labels = y_true_in\n",
        "    y_pred = y_pred_in\n",
        "\n",
        "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=([0,0.5,1], [0,0.5, 1]))\n",
        "\n",
        "    intersection = temp1[0]\n",
        "\n",
        "    area_true = np.histogram(labels,bins=[0,0.5,1])[0]\n",
        "    area_pred = np.histogram(y_pred, bins=[0,0.5,1])[0]\n",
        "    area_true = np.expand_dims(area_true, -1)\n",
        "    area_pred = np.expand_dims(area_pred, 0)\n",
        "\n",
        "    # Compute union\n",
        "    union = area_true + area_pred - intersection\n",
        "\n",
        "    # Exclude background from the analysis\n",
        "    intersection = intersection[1:,1:]\n",
        "    intersection[intersection == 0] = 1e-9\n",
        "\n",
        "    union = union[1:,1:]\n",
        "    union[union == 0] = 1e-9\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou\n",
        "\n",
        "def dice_metric(y_true_in, y_pred_in):\n",
        "    labels = y_true_in\n",
        "    y_pred = y_pred_in\n",
        "\n",
        "    temp1 = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=([0,0.5,1], [0,0.5, 1]))\n",
        "\n",
        "    intersection = temp1[0]\n",
        "\n",
        "    area_true = np.histogram(labels,bins=[0,0.5,1])[0]\n",
        "    area_pred = np.histogram(y_pred, bins=[0,0.5,1])[0]\n",
        "    area_true = np.expand_dims(area_true, -1)\n",
        "    area_pred = np.expand_dims(area_pred, 0)\n",
        "\n",
        "    # Compute union\n",
        "    union = area_true + area_pred\n",
        "\n",
        "    # Exclude background from the analysis\n",
        "    intersection = intersection[1:,1:]\n",
        "    intersection[intersection == 0] = 1e-9\n",
        "\n",
        "    union = union[1:,1:]\n",
        "    union[union == 0] = 1e-9\n",
        "\n",
        "    dice_m = (2*intersection) / union\n",
        "    return dice_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2gbybWx7JTP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "import cv2\n",
        "import warnings\n",
        "import tensorflow\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import *\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "warnings.filterwarnings('ignore')\n",
        "# Path for the stain normalized image patches normalized image\n",
        "test_data_path = '/content/TestData/'\n",
        "# Path to the full sized test mask for score computation\n",
        "gt_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Test/label/'\n",
        "# Path to model weight and weight name\n",
        "model_path = '/content/checkpoint'\n",
        "weight_name = 'nuclei_seg_og.h5'\n",
        "# Path to save the segmented masks\n",
        "if not os.path.exists(\"/content/results\"):\n",
        "    os.mkdir(\"/content/results\")\n",
        "sv_path = '/content/results'\n",
        "\n",
        "# used height and widht for patch\n",
        "img_width_p = 256\n",
        "img_height_p = 256\n",
        "# Full image size\n",
        "img_width_f = 1000\n",
        "img_height_f = 1000\n",
        "\n",
        "test_generator = DataGeneratorFolder(root_dir = test_data_path,\n",
        "                                    image_folder = 'tis/',\n",
        "                                    mask_folder = 'Bin/',\n",
        "                                    batch_size=1,augmentation = None,\n",
        "                                    image_size=img_width_p,\n",
        "                                    nb_y_features = 1)\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "model.load_weights(os.path.join(model_path,weight_name))\n",
        "\n",
        "out_im = []\n",
        "\n",
        "print('Predicting the masks ===========>')\n",
        "for tes in test_generator:\n",
        "    # Xtest_n, y_test_n  = test_generator.__getitem__(tes)\n",
        "    Xtest_n, y_test_n = tes[0], tes[1]\n",
        "    predicted = model.predict(np.expand_dims(Xtest_n[0], axis=0)).reshape(img_width_p, img_height_p)\n",
        "    predicted1= predicted.flatten()\n",
        "    predicted1[predicted1>=0.5]=1\n",
        "    predicted1[predicted1<0.5]=0\n",
        "    predicted2 = predicted1.reshape((img_width_p, img_height_p))\n",
        "    predicted2 = np.expand_dims(predicted2, -1)\n",
        "    out_im.append(predicted2)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Creating full sized segmented image (actual size) from segmented patches\n",
        "print('Joining the segmented patches to original sized masks ===========>')\n",
        "out_full,ids_test = patch_join(out_im)\n",
        "\n",
        "# Writing the masks as image filee to folder\n",
        "print('Writing segmented masks to image files ===========>')\n",
        "for n, id_ in tqdm(enumerate(ids_test), total=len(ids_test)):\n",
        "\n",
        "    imgs = np.reshape(out_full[n]*255,(img_width_f,img_height_f))\n",
        "    filename = '{}/{}.png'.format(sv_path,os.path.splitext(id_)[0])\n",
        "    cv2.imwrite(filename, imgs)\n",
        "\n",
        "print('Segmented images are saved in {}'.format(sv_path))\n",
        "\n",
        "cv2_imshow(out_full[0]*255)\n",
        "\n",
        "# Computing scores (DICE and IOU)\n",
        "print('Scores for the segmented output ===========>')\n",
        "scr_met = {'IOU':[],'DICE':[]}\n",
        "\n",
        "for _,i in enumerate(ids_test):\n",
        "\n",
        "    gt = gt_path+os.path.splitext(i)[0]+'.png'\n",
        "    plabel = os.path.join(sv_path,os.path.splitext(i)[0]+'.png')\n",
        "\n",
        "    true   = cv2.imread(gt,0)\n",
        "    _, true = cv2.threshold(true, 128, 255, cv2.THRESH_BINARY)\n",
        "    true = true.astype(bool)\n",
        "    pred_1 = cv2.imread(plabel,0).astype(bool)\n",
        "\n",
        "    dice_coeff = dice_metric(true,pred_1)\n",
        "    jacc_f = iou_metric(true,pred_1)\n",
        "\n",
        "    scr_met['IOU'].append(jacc_f.item())\n",
        "    scr_met['DICE'].append(dice_coeff.item())\n",
        "    print('ID-{}  IOU: {:.3}, DICE: {:.3}'.format(os.path.splitext(id_)[0],jacc_f.item(),dice_coeff.item()))\n",
        "\n",
        "\n",
        "print(\"mean of jaccard: \",mean(scr_met['IOU']))\n",
        "print(\"mean of dice: \",mean(scr_met['DICE']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWTcbC2_8IL_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "import cv2\n",
        "import warnings\n",
        "import tensorflow\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import *\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "warnings.filterwarnings('ignore')\n",
        "# Path for the stain normalized image patches normalized image\n",
        "test_data_path = '/content/TestData/'\n",
        "# Path to the full sized test mask for score computation\n",
        "gt_path = '/content/drive/MyDrive/NucleiSegNet/Monuseg/Test/label/'\n",
        "# Path to model weight and weight name\n",
        "model_path = '/content/checkpoint'\n",
        "weight_name = 'nuclei_seg_modified.h5'\n",
        "# Path to save the segmented masks\n",
        "if not os.path.exists(\"/content/results\"):\n",
        "    os.mkdir(\"/content/results\")\n",
        "sv_path = '/content/results'\n",
        "\n",
        "# used height and widht for patch\n",
        "img_width_p = 256\n",
        "img_height_p = 256\n",
        "# Full image size\n",
        "img_width_f = 1000\n",
        "img_height_f = 1000\n",
        "\n",
        "test_generator = DataGeneratorFolder(root_dir = test_data_path,\n",
        "                                    image_folder = 'tis/',\n",
        "                                    mask_folder = 'Bin/',\n",
        "                                    batch_size=1,augmentation = None,\n",
        "                                    image_size=img_width_p,\n",
        "                                    nb_y_features = 1)\n",
        "\n",
        "\n",
        "model = create_model_modified()\n",
        "model.load_weights(os.path.join(model_path,weight_name))\n",
        "\n",
        "out_im = []\n",
        "\n",
        "print('Predicting the masks ===========>')\n",
        "for tes in test_generator:\n",
        "    # Xtest_n, y_test_n  = test_generator.__getitem__(tes)\n",
        "    Xtest_n, y_test_n = tes[0], tes[1]\n",
        "    predicted = model.predict(np.expand_dims(Xtest_n[0], axis=0)).reshape(img_width_p, img_height_p)\n",
        "    predicted1= predicted.flatten()\n",
        "    predicted1[predicted1>=0.5]=1\n",
        "    predicted1[predicted1<0.5]=0\n",
        "    predicted2 = predicted1.reshape((img_width_p, img_height_p))\n",
        "    predicted2 = np.expand_dims(predicted2, -1)\n",
        "    out_im.append(predicted2)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Creating full sized segmented image (actual size) from segmented patches\n",
        "print('Joining the segmented patches to original sized masks ===========>')\n",
        "out_full,ids_test = patch_join(out_im)\n",
        "\n",
        "# Writing the masks as image filee to folder\n",
        "print('Writing segmented masks to image files ===========>')\n",
        "for n, id_ in tqdm(enumerate(ids_test), total=len(ids_test)):\n",
        "\n",
        "    imgs = np.reshape(out_full[n]*255,(img_width_f,img_height_f))\n",
        "    filename = '{}/{}.png'.format(sv_path,os.path.splitext(id_)[0])\n",
        "    cv2.imwrite(filename, imgs)\n",
        "\n",
        "print('Segmented images are saved in {}'.format(sv_path))\n",
        "\n",
        "cv2_imshow(out_full[0]*255)\n",
        "\n",
        "# Computing scores (DICE and IOU)\n",
        "print('Scores for the segmented output ===========>')\n",
        "scr_met = {'IOU':[],'DICE':[]}\n",
        "\n",
        "for _,i in enumerate(ids_test):\n",
        "\n",
        "    gt = gt_path+os.path.splitext(i)[0]+'.png'\n",
        "    plabel = os.path.join(sv_path,os.path.splitext(i)[0]+'.png')\n",
        "\n",
        "    true   = cv2.imread(gt,0)\n",
        "    _, true = cv2.threshold(true, 128, 255, cv2.THRESH_BINARY)\n",
        "    true = true.astype(bool)\n",
        "    pred_1 = cv2.imread(plabel,0).astype(bool)\n",
        "\n",
        "    dice_coeff = dice_metric(true,pred_1)\n",
        "    jacc_f = iou_metric(true,pred_1)\n",
        "\n",
        "    scr_met['IOU'].append(jacc_f.item())\n",
        "    scr_met['DICE'].append(dice_coeff.item())\n",
        "    print('ID-{}  IOU: {:.3}, DICE: {:.3}'.format(os.path.splitext(id_)[0],jacc_f.item(),dice_coeff.item()))\n",
        "\n",
        "\n",
        "print(\"mean of jaccard: \",mean(scr_met['IOU']))\n",
        "print(\"mean of dice: \",mean(scr_met['DICE']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q7ARM0RHUBQ"
      },
      "outputs": [],
      "source": [
        "!python overlay_mask.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HSrc-OJ_X3H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}